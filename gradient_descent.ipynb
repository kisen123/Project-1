{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe949043",
   "metadata": {
    "id": "fe949043"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Hint from section 3.3.\n",
    "torch.manual_seed(123)\n",
    "torch.set_default_dtype(torch.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13519814",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7a7f01",
   "metadata": {
    "id": "ad7a7f01"
   },
   "source": [
    "# 1. Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b262fe6f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b262fe6f",
    "outputId": "b90a4465-4932-4ff8-e98e-64bb0b8ed130"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e37c53f6e5b497b8c9450571e3ac3a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/cifar-10-python.tar.gz to ../data/\n",
      "Files already downloaded and verified\n",
      "\n",
      "The mean_std_matrix used for dataset standardization is: \n",
      "\n",
      "[[0.50758954 0.52643419 0.50727447]\n",
      " [0.23966236 0.23350179 0.26788819]]\n",
      "\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Done splitting and normalizing the CIFAR-2 dataset!\n",
      "\n",
      "Size of the training dataset:  9017\n",
      "Size of the validation dataset:  983\n",
      "Size of the test dataset:  2000\n"
     ]
    }
   ],
   "source": [
    "# 1. Loading data\n",
    "def load_cifar(train_val_split=0.9, data_path='../data/', mean_std_matrix=None):\n",
    "    \n",
    "    # This code is based on the code from the weekly exercises\n",
    "    \n",
    "    # Define preprocessor specific for CIFAR-10. The normalization \n",
    "    # is done to better accommodate for faster training.\n",
    "    if mean_std_matrix is None:\n",
    "        preprocessor = transforms.ToTensor()\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        preprocessor = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((mean_std_matrix[0,0], mean_std_matrix[0,1], mean_std_matrix[0,2]),\n",
    "                                (mean_std_matrix[1,0], mean_std_matrix[1,1], mean_std_matrix[1,2]))])\n",
    "        \n",
    "            \n",
    "    # Load datasets.\n",
    "    data_train_val = datasets.CIFAR10(\n",
    "        data_path,       \n",
    "        train=True,      \n",
    "        download=True,\n",
    "        transform=preprocessor)\n",
    "\n",
    "    data_test = datasets.CIFAR10(\n",
    "        data_path, \n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=preprocessor)\n",
    "\n",
    "    # Train/Validation split.\n",
    "    n_train = int(len(data_train_val)*train_val_split)\n",
    "    n_val =  len(data_train_val) - n_train\n",
    "\n",
    "    data_train, data_val = random_split(\n",
    "        data_train_val, \n",
    "        [n_train, n_val],\n",
    "        generator=torch.Generator().manual_seed(123)\n",
    "    )\n",
    "    return (data_train, data_val, data_test)\n",
    "\n",
    "# Collecting the training set to be able to calculate the mean and std values\n",
    "# for normalization\n",
    "cifar10_train, _, _ = load_cifar()\n",
    "\n",
    "# Now we define a lighter version of CIFAR-10, CIFAR-2.\n",
    "label_map = {0: 0, 2: 1}\n",
    "class_names = ['airplane', 'bird']\n",
    "\n",
    "# For each dataset, keep only airplanes and birds.\n",
    "cifar2_train = [(img, label_map[label]) for img, label in cifar10_train if label in [0, 2]]\n",
    "cifar2_imgs = [train_tuple[0].numpy() for train_tuple in cifar2_train]\n",
    "\n",
    "# We calculate the training set's mean and std for each rgb channel\n",
    "means = np.mean(cifar2_imgs, axis=(0, 2, 3))\n",
    "stds = np.std(cifar2_imgs, axis=(0, 2, 3))\n",
    "mean_std_matrix = np.array([means, stds])\n",
    "\n",
    "print('\\nThe mean_std_matrix used for dataset standardization is: \\n')\n",
    "print(str(mean_std_matrix) + '\\n')\n",
    "\n",
    "# With the mean and std values, we collect the appropriately normalized CIFAR-2 evaluation sets\n",
    "cifar10_train, cifar10_val, cifar10_test = load_cifar(mean_std_matrix=mean_std_matrix)\n",
    "\n",
    "\n",
    "cifar2_val = [(img, label_map[label]) for img, label in cifar10_val if label in [0, 2]]\n",
    "cifar2_test = [(img, label_map[label]) for img, label in cifar10_test if label in [0, 2]]\n",
    "\n",
    "print('\\nDone splitting and normalizing the CIFAR-2 dataset!\\n')\n",
    "print('Size of the training dataset: ', len(cifar2_train))\n",
    "print('Size of the validation dataset: ', len(cifar2_val))\n",
    "print('Size of the test dataset: ', len(cifar2_test))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baa4f44",
   "metadata": {
    "id": "5baa4f44"
   },
   "source": [
    "# 2. Defining a Multi-Layer Perceptron in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aead8e83",
   "metadata": {
    "id": "aead8e83"
   },
   "outputs": [],
   "source": [
    "# 2. Defining a Multi-Layer Perceptron in PyTorch\n",
    "class MyMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Flattening the input so as to have 32*32*3 neurons as input in a one-\n",
    "        # dimensional vector.\n",
    "        self.L = 4\n",
    "        self.flat = nn.Flatten() \n",
    "\n",
    "        # The instructions from the task are set.\n",
    "        self.fc1 = nn.Linear(32*32*3, 512)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(128, 32)\n",
    "        self.act3 = nn.ReLU()\n",
    "\n",
    "        self.fc4 = nn.Linear(32, 2)\n",
    "        # Leaving out an activation function for the last layer according to the\n",
    "        # project description.\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # Forward-propagation through the network.\n",
    "        out = self.flat(x)\n",
    "        out = self.act1(self.fc1(out))\n",
    "        out = self.act2(self.fc2(out))\n",
    "        out = self.act3(self.fc3(out))\n",
    "        out = self.fc4(out)\n",
    "        \n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e7fe83",
   "metadata": {
    "id": "68e7fe83"
   },
   "source": [
    "# 3. train()-function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "681d5f8e",
   "metadata": {
    "id": "681d5f8e"
   },
   "outputs": [],
   "source": [
    "# 3. train()-function\n",
    "def train(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    \n",
    "    # Setting a few standard variables and empty lists.\n",
    "    n_batch = len(train_loader)\n",
    "    losses_train = []\n",
    "    \n",
    "    # The model is set to be trained by a PyTorch method.\n",
    "    model.train()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        \n",
    "        loss_train = 0.0\n",
    "        \n",
    "        # This for-loop calculates loss and makes a gradient descent step.\n",
    "        for imgs, labels in train_loader:\n",
    "            \n",
    "            # Following the hint in section 3.3.\n",
    "            imgs = imgs.to(device=device, dtype=torch.double) \n",
    "            labels = labels.to(device=device)\n",
    "            \n",
    "            # Forward-propagates the input.\n",
    "            outputs = model(imgs)\n",
    "            \n",
    "            # The loss is computed, and a backward-propagation is done.\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient descent is done, and the gradients are zeroed out for\n",
    "            # each minibatch.\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # The training losses are stored for each gradient descent step.\n",
    "            loss_train += loss.item()\n",
    "            \n",
    "        # The average of the losses computed for one epoch. \n",
    "        losses_train.append(loss_train/n_batch)\n",
    "\n",
    "        # Motivational text keeping the user updated on the progress.\n",
    "        if epoch%10 == 0 or epoch == 1:\n",
    "            print('{}  |  Epoch {}  |  Training loss {:.3f}'.format(\n",
    "                datetime.now().time(), epoch, loss_train/n_batch))\n",
    "    return losses_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c328ae20",
   "metadata": {
    "id": "c328ae20"
   },
   "source": [
    "# 4. Manual training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f49c8f3a",
   "metadata": {
    "id": "f49c8f3a"
   },
   "outputs": [],
   "source": [
    "# 4. Manual training function\n",
    "def train_manual_update(n_epochs, lr, model, loss_fn, train_loader):\n",
    "    \n",
    "    # Setting a few standard variables and empty lists.\n",
    "    n_batch = len(train_loader)\n",
    "    losses_train = []\n",
    "    \n",
    "    # The model is set to be trained by a PyTorch method.\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            \n",
    "            # Following the hint in section 3.3.\n",
    "            imgs = imgs.to(device=device, dtype=torch.double) \n",
    "            labels = labels.to(device=device)\n",
    "                \n",
    "            # Forward-propagates the input.\n",
    "            outputs = model(imgs)\n",
    "                \n",
    "            # The loss is computed, and a backward-propagation is done.\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                # Iterating over the model parameters.\n",
    "                for p in model.parameters():\n",
    "                    \n",
    "                    # Applying the formula for gradient descent.\n",
    "                    p.data = p.data - lr * p.grad\n",
    "                    p.grad = torch.zeros_like(p.grad)\n",
    "\n",
    "            # The training losses are stored for each gradient descent step.\n",
    "            loss_train += loss.item()\n",
    "        \n",
    "        # The average of the losses computed for one epoch. \n",
    "        losses_train.append(loss_train/n_batch)\n",
    "\n",
    "        # Motivational text keeping the user updated on the progress.\n",
    "        if epoch%10 == 0 or epoch == 1:\n",
    "            print('{}  |  Epoch {}  |  Training loss {:.3f}'.format(\n",
    "                datetime.now().time(), epoch, loss_train / n_batch))    \n",
    "    return losses_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf54779a",
   "metadata": {
    "id": "bf54779a"
   },
   "source": [
    "# 5. Training 2 instances of MyMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b8a17a9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5b8a17a9",
    "outputId": "b38b3d88-2288-4946-db7a-112ba4904ae7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cpu.\n",
      "21:44:41.755841  |  Epoch 1  |  Training loss 0.695\n",
      "21:44:53.867573  |  Epoch 10  |  Training loss 0.581\n",
      "21:45:07.845948  |  Epoch 20  |  Training loss 0.502\n",
      "21:45:09.428939  |  Epoch 1  |  Training loss 0.695\n",
      "21:45:24.170698  |  Epoch 10  |  Training loss 0.581\n",
      "21:45:40.033089  |  Epoch 20  |  Training loss 0.502\n",
      "The train-functions seem to output the same models!\n"
     ]
    }
   ],
   "source": [
    "# 5. Training 2 instances of MyMLP\n",
    "\n",
    "# We define a function for verifying similarity of training losses. This takes\n",
    "# care of numerical nuances.\n",
    "def relative_error(a, b):\n",
    "    return (a - b) / a\n",
    "\n",
    "# Evaluating the processing unit to train on.\n",
    "device = (torch.device('cuda') if torch.cuda.is_available()\n",
    "          else torch.device('cpu'))\n",
    "print(f\"Training on device {device}.\")\n",
    "\n",
    "# The train loader is set with an appropriate batch size.\n",
    "train_loader = torch.utils.data.DataLoader(cifar2_train, batch_size=256, shuffle=False)\n",
    "\n",
    "# A seed must be set before each instantiation of the model for reproducibility.\n",
    "torch.manual_seed(123)\n",
    "model = MyMLP().to(device=device)\n",
    "\n",
    "# The optimizer and the loss function is set.\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# This function utilizes PyTorch's already written code for the SGD optimizer.\n",
    "losses_train_SGD = train(\n",
    "    n_epochs = 20,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader\n",
    ")\n",
    "\n",
    "# Re-instantiating model = MyMLP() with the same seed.\n",
    "torch.manual_seed(123)\n",
    "model = MyMLP().to(device=device)\n",
    "\n",
    "# The manual gradient descent algorithm is contained in this training function.\n",
    "losses_train_manual = train_manual_update(\n",
    "    n_epochs = 20,\n",
    "    lr = 1e-2,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader\n",
    ")\n",
    "# This code checks whether the models coincide, where a threshold is \n",
    "# set to classify numerical losses.\n",
    "list_of_trues = []\n",
    "for i, _ in enumerate(losses_train_SGD):\n",
    "    if abs(relative_error(losses_train_SGD[i], losses_train_manual[i])) <= 10**(-10):\n",
    "        list_of_trues.append(True)\n",
    "if len(list_of_trues) == len(losses_train_SGD):\n",
    "    print(\"The train-functions seem to output the same models!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe211bcd",
   "metadata": {
    "id": "fe211bcd"
   },
   "source": [
    "# 6. Adding regularization/weight decay to the manual train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b291061",
   "metadata": {
    "id": "3b291061"
   },
   "outputs": [],
   "source": [
    "# 6. Adding regularization/weight decay to the manual train function\n",
    "def train_manual_update_with_L2(n_epochs, lr, model, loss_fn, train_loader, weight_decay):\n",
    "    \n",
    "    # Setting a few standard variables and empty lists.\n",
    "    n_batch = len(train_loader)\n",
    "    losses_train = []\n",
    "    \n",
    "    # The model is set to be trained by a PyTorch method.\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        \n",
    "        loss_train = 0.0\n",
    "        \n",
    "        for imgs, labels in train_loader:\n",
    "\n",
    "            # Following the hint in section 3.3.\n",
    "            imgs = imgs.to(device=device, dtype=torch.double) \n",
    "            labels = labels.to(device=device)\n",
    "            \n",
    "            # Forward-propagates the input.\n",
    "            outputs = model(imgs)\n",
    "            \n",
    "            # The losses are computed, and a backward-propagation is done.\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                # Iterating over the model parameters\n",
    "                for p in model.parameters():\n",
    "                    \n",
    "                    # Applying the formula for weight-decay/L2-regularization\n",
    "                    # Note that the lambda has been subsituted by \n",
    "                    # a hyperparameter weight_decay as the .pdf-file suggests.\n",
    "                    # (see the parenthesis). \n",
    "                    # Applying the formula for gradient descent in the same line.\n",
    "                    p.data = p.data - lr * (p.grad + weight_decay * p.data)\n",
    "                    p.grad = torch.zeros_like(p.grad)\n",
    "            \n",
    "            # The training losses are stored for each gradient descent step.\n",
    "            loss_train += loss.item()\n",
    "            \n",
    "        # The average of the losses computed for one epoch.\n",
    "        losses_train.append(loss_train/n_batch)\n",
    "\n",
    "        # Motivational text keeping the user updated on the progress.\n",
    "        if epoch%10 == 0 or epoch == 1:\n",
    "            print('{}  |  Epoch {}  |  Training loss {:.3f}'.format(\n",
    "            datetime.now().time(), epoch, loss_train / n_batch))    \n",
    "    return losses_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2cb7cb1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c2cb7cb1",
    "outputId": "f245541d-1aa7-4113-8fb5-c4c7c6c0660c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cpu.\n",
      "21:46:34.853788  |  Epoch 1  |  Training loss 0.695\n",
      "21:46:48.595777  |  Epoch 10  |  Training loss 0.593\n",
      "21:47:03.543391  |  Epoch 20  |  Training loss 0.506\n",
      "21:47:05.473513  |  Epoch 1  |  Training loss 0.695\n",
      "21:47:23.069549  |  Epoch 10  |  Training loss 0.593\n",
      "21:47:42.295341  |  Epoch 20  |  Training loss 0.506\n",
      "The train-functions seem to output the same models!\n"
     ]
    }
   ],
   "source": [
    "#%% This section runs the code for the same inputs as previous models, but now with a weight decay term.\n",
    "# We define a function for verifying similarity of training losses. This takes\n",
    "# care of numerical nuances.\n",
    "def relative_error(a, b):\n",
    "    return (a - b) / a\n",
    "\n",
    "# Evaluating the processing unit to train on.\n",
    "device = (torch.device('cuda') if torch.cuda.is_available()\n",
    "          else torch.device('cpu'))\n",
    "print(f\"Training on device {device}.\")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(cifar2_train, batch_size=256, shuffle=False)\n",
    "\n",
    "\n",
    "# Seed must be set before each instantiation of the model.\n",
    "torch.manual_seed(123)\n",
    "model = MyMLP().to(device=device)\n",
    "\n",
    "# The optimizer and the loss function is set.\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2, weight_decay=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# This function utilizes PyTorch's already written code for the SGD optimimzer.\n",
    "losses_train_SGD = train(\n",
    "    n_epochs = 20,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader\n",
    ")\n",
    "\n",
    "# Re-instantiating model = MyMLP() with the same seed.\n",
    "torch.manual_seed(123)\n",
    "model = MyMLP().to(device=device)\n",
    "\n",
    "# The manual gradient descent algorithm is contained in this training function.\n",
    "losses_train_manual_weight_decay = train_manual_update_with_L2(\n",
    "    n_epochs = 20,\n",
    "    lr = 1e-2,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    "    weight_decay = 0.01\n",
    ")\n",
    "\n",
    "# This code checks whether the models coincide, where a threshold is \n",
    "# set to classify numerical losses.\n",
    "list_of_trues = []\n",
    "for i, _ in enumerate(losses_train_SGD):\n",
    "    if abs(relative_error(losses_train_SGD[i], losses_train_manual_weight_decay[i])) <= 10**(-10):\n",
    "        list_of_trues.append(True)\n",
    "if len(list_of_trues) == len(losses_train_SGD):\n",
    "    print(\"The train-functions seem to output the same models!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e9513d",
   "metadata": {
    "id": "92e9513d"
   },
   "source": [
    "# 7. Adding momentum to the manual training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18bbe3fa",
   "metadata": {
    "id": "18bbe3fa"
   },
   "outputs": [],
   "source": [
    "#%% 7. Adding momentum to the manual training function\n",
    "def train_manual_update_with_L2_decay_momentum(n_epochs, lr, model, loss_fn, train_loader, weight_decay, momentum_coeff):\n",
    "    \n",
    "    # Setting a few standard variables and empty lists\n",
    "    n_batch = len(train_loader)\n",
    "    losses_train = []\n",
    "    start_gradients = []\n",
    "    \n",
    "    # We follow Andrew's notations here for the momentum gradients. NOTE:\n",
    "    # vdW_vdb implies derivatives wrt weights and biases.\n",
    "    vdW_vdb = []\n",
    "    \n",
    "    # The model is set to be trained by a PyTorch method\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        \n",
    "        loss_train = 0.0\n",
    "        \n",
    "        for imgs, labels in train_loader:\n",
    "\n",
    "            # Following the hint in section 3.3.\n",
    "            imgs = imgs.to(device=device, dtype=torch.double) \n",
    "            labels = labels.to(device=device)\n",
    "            \n",
    "            # Forward-propagates the input\n",
    "            outputs = model(imgs)\n",
    "            \n",
    "            # The losses are computed, and a backward-propagation is done.\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                # Iterating over the model parameters\n",
    "                # Here we need the index i for the momentum calculation.\n",
    "                for i, p in enumerate(model.parameters()):\n",
    "                    \n",
    "                    # Applying the formula for weight-decay/L2-regularization\n",
    "                    # Note that the lambda has been subsituted by \n",
    "                    # a hyperparameter weight_decay as the .pdf-file suggests.\n",
    "                    p.grad = p.grad + weight_decay * p.data\n",
    "                    \n",
    "                    # These if and else statements will store the gradients\n",
    "                    # for the first minibatch.\n",
    "                    if len(vdW_vdb) == 0:\n",
    "                        if len(start_gradients) == 0:\n",
    "                            start_gradients = [p.grad]\n",
    "                        else:\n",
    "                            start_gradients.append(p.grad)\n",
    "                    else:\n",
    "                        \n",
    "                        # NOTE! The (1 - momentum_coeff) term has been omitted,\n",
    "                        # because that gave the same result as the train()-\n",
    "                        # function with the SGD optimizer from PyTorch.\n",
    "                        \n",
    "                        # Updating the gradient of the weights and biases\n",
    "                        # in their respective places in the network (i denotes\n",
    "                        # where in the network we are).\n",
    "                        p.grad = vdW_vdb[i] * momentum_coeff + p.grad\n",
    "                        vdW_vdb[i] = p.grad\n",
    "                        \n",
    "                    \n",
    "                    # Applying the formula for gradient descent.\n",
    "                    p.data = p.data - lr * p.grad\n",
    "                    p.grad = torch.zeros_like(p.grad)\n",
    "            \n",
    "            # This if-statement is only satisfied for the first minibatch: it stores\n",
    "            # the gradients of the first minibatch such that the gradients for\n",
    "            # the second mini-batch can build upon the \"first gradients\" for momentum\n",
    "            # calculation. \n",
    "            if len(vdW_vdb) == 0:\n",
    "                vdW_vdb = start_gradients\n",
    "            \n",
    "            # The training losses are stored for each gradient descent step.\n",
    "            loss_train += loss.item() \n",
    "            \n",
    "        # The average of the losses computed for one epoch.\n",
    "        losses_train.append(loss_train/n_batch)\n",
    "\n",
    "        # Motivational text keeping the user updated on the progress.\n",
    "        if epoch%10 == 0 or epoch == 1:\n",
    "            print('{}  |  Epoch {}  |  Training loss {:.3f}'.format(\n",
    "            datetime.now().time(), epoch, loss_train / n_batch))\n",
    "    return losses_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bb0b1ec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0bb0b1ec",
    "outputId": "b36d34b6-91cf-437b-df8d-fd3d1aec96f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cpu.\n",
      "21:48:19.329496  |  Epoch 1  |  Training loss 0.680\n",
      "21:48:33.109078  |  Epoch 10  |  Training loss 0.456\n",
      "21:48:48.863826  |  Epoch 20  |  Training loss 0.388\n",
      "21:48:51.394392  |  Epoch 1  |  Training loss 0.680\n",
      "21:49:12.080204  |  Epoch 10  |  Training loss 0.456\n",
      "21:49:34.543091  |  Epoch 20  |  Training loss 0.388\n",
      "The train-functions seem to output the same models!\n"
     ]
    }
   ],
   "source": [
    "#%% This section runs the code for the same inputs as previous models, but now with a weight decay term and a momentum coefficient.\n",
    "# We define a function for verifying similarity of training losses. This takes\n",
    "# care of numerical nuances.\n",
    "def relative_error(a, b):\n",
    "    return (a - b) / a\n",
    "\n",
    "# Evaluating the processing unit to train on.\n",
    "device = (torch.device('cuda') if torch.cuda.is_available()\n",
    "          else torch.device('cpu'))\n",
    "print(f\"Training on device {device}.\")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(cifar2_train, batch_size=256, shuffle=False)\n",
    "\n",
    "\n",
    "# Seed must be set before each instantiation of the model.\n",
    "torch.manual_seed(123)\n",
    "model = MyMLP().to(device=device)\n",
    "\n",
    "# The optimizer and the loss function is set.\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2, weight_decay=0.01, momentum=0.9)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# This function utilizes PyTorch's already written code for the SGD optimimzer.\n",
    "losses_train_SGD = train(\n",
    "    n_epochs = 20,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader\n",
    ")\n",
    "\n",
    "# Re-instantiating model = MyMLP() with the same seed.\n",
    "torch.manual_seed(123)\n",
    "model = MyMLP().to(device=device)\n",
    "\n",
    "# The manual gradient descent algorithm is contained in this training function.\n",
    "losses_train_manual_weight_decay_momentum = train_manual_update_with_L2_decay_momentum(\n",
    "    n_epochs = 20,\n",
    "    lr = 1e-2,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    "    weight_decay = 0.01,\n",
    "    momentum_coeff = 0.9\n",
    ")\n",
    "\n",
    "# This code checks whether the models coincide, where a threshold is \n",
    "# set to classify numerical losses.\n",
    "list_of_trues = []\n",
    "for i, _ in enumerate(losses_train_SGD):\n",
    "    if abs(relative_error(losses_train_SGD[i], losses_train_manual_weight_decay_momentum[i])) <= 10**(-10):\n",
    "        list_of_trues.append(True)\n",
    "if len(list_of_trues) == len(losses_train_SGD):\n",
    "    print(\"The train-functions seem to output the same models!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaa4321",
   "metadata": {
    "id": "fdaa4321"
   },
   "source": [
    "# 8. Training multiple instances with different parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "caebc63b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "caebc63b",
    "outputId": "c083310e-ac0e-4ea7-bf87-cbc03cb9f9e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many models would you like to train? 10\n",
      "Training on device cpu.\n",
      "22:03:44.779596  |  Epoch 1  |  Training loss 0.693\n",
      "22:04:07.025673  |  Epoch 10  |  Training loss 0.480\n",
      "MyMLP_1, lr = 0.00158, weight_decay = 0.00042, momentum_coeff = 0.99803\n",
      "22:04:09.522414  |  Epoch 1  |  Training loss 0.699\n",
      "22:04:29.218217  |  Epoch 10  |  Training loss 0.689\n",
      "MyMLP_2, lr = 0.00123, weight_decay = 0.00016, momentum_coeff = 0.11042\n",
      "22:04:31.450573  |  Epoch 1  |  Training loss 0.694\n",
      "22:04:51.992843  |  Epoch 10  |  Training loss 0.693\n",
      "MyMLP_3, lr = 0.02216, weight_decay = 0.63789, momentum_coeff = 0.60163\n",
      "22:04:54.277988  |  Epoch 1  |  Training loss 0.691\n",
      "22:05:14.069675  |  Epoch 10  |  Training loss 0.593\n",
      "MyMLP_4, lr = 0.00928, weight_decay = 0.15813, momentum_coeff = 0.99894\n",
      "22:05:16.296145  |  Epoch 1  |  Training loss 0.718\n",
      "22:05:36.363058  |  Epoch 10  |  Training loss 0.694\n",
      "MyMLP_5, lr = 0.16921, weight_decay = 0.00157, momentum_coeff = 0.93284\n",
      "22:05:38.716337  |  Epoch 1  |  Training loss 0.698\n",
      "22:05:59.411150  |  Epoch 10  |  Training loss 0.694\n",
      "MyMLP_6, lr = 0.0008, weight_decay = 0.95404, momentum_coeff = 0.64635\n",
      "22:06:01.584000  |  Epoch 1  |  Training loss 0.671\n",
      "22:06:21.614396  |  Epoch 10  |  Training loss 0.693\n",
      "MyMLP_7, lr = 0.3599, weight_decay = 0.10217, momentum_coeff = 0.19266\n",
      "22:06:23.867232  |  Epoch 1  |  Training loss 0.697\n",
      "22:06:43.483814  |  Epoch 10  |  Training loss 0.656\n",
      "MyMLP_8, lr = 0.00123, weight_decay = 0.04364, momentum_coeff = 0.83782\n",
      "22:06:45.736566  |  Epoch 1  |  Training loss 0.698\n",
      "22:07:06.461655  |  Epoch 10  |  Training loss 0.528\n",
      "MyMLP_9, lr = 0.00023, weight_decay = 0.00012, momentum_coeff = 0.99261\n",
      "22:07:08.774228  |  Epoch 1  |  Training loss 0.702\n",
      "22:07:28.478762  |  Epoch 10  |  Training loss 0.496\n",
      "MyMLP_10, lr = 0.09193, weight_decay = 0.00462, momentum_coeff = 0.9502\n"
     ]
    }
   ],
   "source": [
    "#%% 8. Training multiple instances with different parameters\n",
    "\n",
    "# Setting a few standard variables and empty lists.\n",
    "models = []\n",
    "model_names = []\n",
    "losses_train = []\n",
    "losses_val = []\n",
    "\n",
    "# Asking the user for the number of inputs.\n",
    "answer = int(input(\"How many models would you like to train? \"))\n",
    "\n",
    "device = (torch.device('cuda') if torch.cuda.is_available()\n",
    "          else torch.device('cpu'))\n",
    "print(f\"Training on device {device}.\")\n",
    "\n",
    "# This will set the random numbers inside the for loop to not be random for \n",
    "# reproducibility. \n",
    "np.random.seed(666)\n",
    "\n",
    "for i in range(1, answer+1):\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(cifar2_train, batch_size=256, shuffle=False)\n",
    "    torch.manual_seed(123)\n",
    "    model = MyMLP().to(device=device) \n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # The parameters are made such that the model does a random search instead\n",
    "    # of a grid search as Andrew's videos suggest.\n",
    "    lr = 10**(-4*np.random.rand())\n",
    "    weight_decay = 10**(-4*np.random.rand())\n",
    "    momentum_coeff = 1 - 10**(-4*np.random.rand())\n",
    "    \n",
    "    train_manual_update_with_L2_decay_momentum(n_epochs=10,\n",
    "                                               lr=lr,\n",
    "                                               model=model,\n",
    "                                               loss_fn=loss_fn,\n",
    "                                               train_loader=train_loader,\n",
    "                                               weight_decay=weight_decay, \n",
    "                                               momentum_coeff=momentum_coeff)\n",
    "    \n",
    "    # Storing the models for later use.\n",
    "    models.append(model)\n",
    "    model_names.append('MyMLP_' + str(i) + ', lr = ' + str(round(lr, 5)) + ', weight_decay = ' + str(round(weight_decay, 5)) + ', momentum_coeff = ' + str(round(momentum_coeff, 5)))\n",
    "    print(model_names[i-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fb1083",
   "metadata": {
    "id": "b8fb1083"
   },
   "source": [
    "# 9. Selecting the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a4b8d9a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3a4b8d9a",
    "outputId": "c93cf9d5-287d-4658-c32e-b5cbedac921e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7375\n",
      "Accuracy: 0.4954\n",
      "Accuracy: 0.5046\n",
      "Accuracy: 0.7508\n",
      "Accuracy: 0.5158\n",
      "Accuracy: 0.4954\n",
      "Accuracy: 0.5046\n",
      "Accuracy: 0.7538\n",
      "Accuracy: 0.7518\n",
      "Accuracy: 0.6460\n",
      "\n",
      "The best model is  MyMLP_8, lr = 0.00123, weight_decay = 0.04364, momentum_coeff = 0.83782\n",
      "Training accuracy of the best model: \n",
      "Accuracy: 0.7141\n",
      "Validation accuracy of the best model: \n",
      "Accuracy: 0.7538\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7538148524923703"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%% 9. Selecting the best model\n",
    "def compute_accuracy(model, loader):\n",
    "    # This code has been pulled from the Week 06 - Machine Learning pipeline\n",
    "    # and MLP.\n",
    "    \n",
    "    # Setting the model to evaluation mode.\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in loader:\n",
    "            imgs = imgs.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "                \n",
    "            # Doing a forward pass of the validation data\n",
    "            outputs = model(imgs)\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            total += labels.shape[0]\n",
    "            correct += int((predicted == labels).sum())\n",
    "\n",
    "    acc =  correct/total\n",
    "    print(\"Accuracy: {:.4f}\".format(acc))\n",
    "    return acc\n",
    "\n",
    "# Setting the train and validation loaders.\n",
    "train_loader = torch.utils.data.DataLoader(cifar2_train, batch_size=256, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=256, shuffle=False)\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "# Checking the validation accuracy of each trained model.\n",
    "for model in models:\n",
    "    accuracies.append(compute_accuracy(model, val_loader))\n",
    "    \n",
    "# Get the index of the best model in the model-list.\n",
    "i_best_model = np.argmax(accuracies)\n",
    "best_model = models[i_best_model]\n",
    "\n",
    "# Revealing the best hyperparameters resulting in the best validation accuracy.\n",
    "print(\"\\nThe best model is \", model_names[i_best_model])\n",
    "\n",
    "# Printing the training and validation accuracies of the best model.\n",
    "print(\"Training accuracy of the best model: \")\n",
    "compute_accuracy(best_model, train_loader)\n",
    "print(\"Validation accuracy of the best model: \")\n",
    "compute_accuracy(best_model, val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423cbad0",
   "metadata": {
    "id": "423cbad0"
   },
   "source": [
    "# 10. Evaluating the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4f7ee60",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c4f7ee60",
    "outputId": "878dc89e-36d5-4348-b703-2e4ea77aa89d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy of the best model: \n",
      "Accuracy: 0.7395\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7395"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%% 10. Evaluating the best model\n",
    "\n",
    "# Setting the test loaders.\n",
    "test_loader = torch.utils.data.DataLoader(cifar2_test, batch_size=256, shuffle=False)\n",
    "\n",
    "# Printing the test accuracy of the best model.\n",
    "print(\"Test accuracy of the best model: \")\n",
    "compute_accuracy(best_model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Section3CodeFINAL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
